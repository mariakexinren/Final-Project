---
title: "Music, Emotions"
author: "Maria Ren"
date: "5/6/2018"
output: 
  pdf_document: 
    latex_engine: xelatex
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Introduction


We often say, “I’m in the mood for a happy tune.” But what exactly makes a song happy? Or sad? 

In the twenty-first century, music has become one of the most important parts of our lives. Whether it’s classical, jazz, funk, pop, or country, the demand for music has created a billion dollar music industry worldwide. Music industry revenue in turn became a significant component in the global economy, which also led to fierce competitions across record companies and other music platforms. These companies strives to satisfy the listener’s taste by creating music that fits individual tastes, 

Among these music platforms, Spotify is one of the most popular digital music services that provides its users with access to millions of songs. It also gives users the ability to rate music according to their own personal preference and taste. 

This project uses real time and historical data extracted from the Spotify API (Developer Portal), and analyzes the different elements of a soundtrack that changes human emotions. The musical data consists of musical features and ratings for the number one songs from “Billboard Hot 100 Chart” from 1960 to 2018.  



# Audio Features: 

The following audio features are extracted from the Spotify API, and used in the data analytics.

####Danceability: 
Describes how a song track is suitable for dancing. Values ranges from 0 to 1, with 0 being the least danceable, and 1 being the most danceable. Danceability is calculated based on music tempo, rhythm stability, beat strength. 

####Energy: 
Describes level of music intensity and activity. For example, the most energetic music feels fast and loud (like death metal), while a quiet and peaceful classical music (like Chopin) would score lower on the energy scale. Calculated from loudness, onset rate, tempo, and dynamic ranges. Values ranges from 0 to 1, with 0 being least energetic, and 1 being most energetic. 

####Loudness: 
Describes the decibel (dB) level of a score. Values ranges from -60 to 0. With -60 being the loudest, and 0 being quietest.

####Speechiness: 
Describes the amount of spoken words in a track. Values ranges from 0 to 1, with 0 being music having no words, and 1 being completely spoken song track. (For example, a talk show would have closer to 1, song with rap with having values above 0.66, values between 0.33 and 0.66 would be regular songs with lyrics, and values below 0.33 are most likely classical music that is completely instrumental). 

####Acousticness: 
Describes the level of acoustic sound (as opposed to electronic sound) detected from the soundtrack. Values ranges from 0 to 1, with 0 being entirely electronic sound, and 1 being acoustic sound. 

####Instrumentalness: 
Describes whether the track contains vocals. ( “Ooh” and “aah” sound are treated as instrumental). Values ranges from 0 to 1, with 0 being the soundtrack having no instrumentation, and 1 being instrumental music. (For example, a rap song would have score closer to 0, and classical symphonic music would have value closer to 1). 

####Liveness: 
Describes whether the song track was recorded live or in a studio. Calculated from detection of audience sound in the sound track. Values ranges from 0 to 1, with values closer to 0 indicating music which were recorded in a studio setting, and with values above 0.8 indicating music recorded live. 

####Valence: 
Describes the happiness level of a music soundtrack. Values ranges from 0 to 1, with 0 being music tracks that are negative (sad, depressed, and angry), while 1 being music that are positive (happy, exciting and cheerful). Calculated from audience reviews and ratings. 

####Tempo: 
Describes the tempo of a track in beats per minute (BPM). Values ranges from 0 to around 200. With values around 50 being very slow, and around 200 being very fast.

####Duration(Duration_ms): 
Describes the length of the soundtrack. Calculated in milliseconds. 

#Data Cleaning  

```{r,include=FALSE}
library(devtools)
library(spotifyr)
library(tidyverse)
library(dplyr)
library(billboard)
library(httr)
library(miscTools)
library(ggplot2)
library(knitr)
library(kableExtra)

```


- Obtain Spotify access token 
- Set Spotify client ID and client secret 
- Obtain real time data from Spotify     

```{r,eval=FALSE}
Sys.setenv(SPOTIFY_CLIENT_ID = '0da3921447fa4e3099913296e99b43d6')
Sys.setenv(SPOTIFY_CLIENT_SECRET = '216f8762ba494bb7a626becb67d6a0ab')
access_token <- get_spotify_access_token()
```


- Obtain historical data set: Billboards Hot 100 list (containing top 100 most 
popular songs from 1960 to 2016. This dataset comes from the R library "billboard", and the data was scraped from 
- <https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_>
- <https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_1960>    



```{r,echo=FALSE}
historical <- wiki_hot_100s %>% 
  rename(track_name = "title") 

kable(historical[1:15,], "latex") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))%>%
  footnote(general = "(The table shows the first 15 rows of the dataset)")
```

- Filter out top song from each year from 1960-2016     

```{r,echo=FALSE}
top <- as.tibble(historical) %>% filter(no=="1") %>% 
  select(track_name, artist, year)
```


- Form a complete list of top song tracks from 1960 to 2018  
```{r,echo=FALSE}
top_complete <- top %>% 
  add_row(track_name = "Shape of You", artist = "Ed Sheeran", year = "2017") %>% 
  as.tibble()

top_complete <- top_complete %>% 
  add_row(track_name = "Perfect", artist = "Ed Sheeran", year = "2018") %>% 
  as.tibble()
kable(top_complete[1:15,], "latex") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))%>%
  footnote(general = "(The table shows the first 15 rows of the dataset)")
```


- Clean up and tidying data
- Include track features into the table       
```{r,echo=FALSE}
track <- spotify_track_data %>% rename(artist  = 'artist_name')
update_track_name <- gsub("\\-.*","",track$track_name) %>%
  as.tibble() 
update_track_name[1,] <- "Theme From A Summer Place"
track$track_name <- update_track_name
track$artist[1] <- "Percy Faith"
track$track_name <- as.tibble(track$track_name)

join_list <- left_join(top_complete, track, by = c("artist","year"))
list <- distinct(join_list,year, .keep_all=TRUE)
list <- subset(list, select = -c(track_name.y,artist_id,explicit,type,uri,track_href,analysis_url,key,mode,time_signature)) %>% 
  rename(track_name = "track_name.x",track_uri = "track_id")
```


- Still need data for the following song tracks  

```{r,echo=FALSE}
incomplete_index <- which(is.na(list$track_uri), arr.ind=TRUE)
incomplete <- list[incomplete_index,] %>% select(track_name,artist,year)
kable(incomplete[,1], "latex") %>%
  kable_styling(bootstrap_options = c("striped", "hover", "condensed"))
```


- Pull data from Spotify API 
- Write functions get_track and get_features
- get_track is a function used to obtain track_uri from Spotify
- get_features is a function used to obtain track features by matching track_uri with track needed for analysis   
```{r,eval=FALSE}
get_track <- function(artist) {
  artists <- get_artists(artist)
  albums <- get_albums(artists$artist_uri[1])
  return(get_album_tracks(albums))
}

get_features <- function(artist) {
  artists <- get_artists(artist)
  albums <- get_albums(artists$artist_uri[1])
  tracks <- get_album_tracks(albums)
  artists_audio_features <- get_track_audio_features(tracks)
  return(artists_audio_features)
}
```


- Obtaining audio features for the incomplete song tracks  

```{r,eval= FALSE}

# track name: Sugar Shack
# artist: The Fireballs 
# year: 1963

song_list <- get_track("The Fireballs")
feature_list <- get_features("The Fireballs") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Sugar Shack")
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data)
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Sugar Shack"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}


# track name: Wooly Bully
# artist: Sam the Sham and the Pharaohs
# year: 1965 

song_list <- get_track("Sam the Sham and the Pharaohs")
feature_list <- get_features("Sam the Sham and the Pharaohs") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Wooly Bully")
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri) 
get_feature_data <- cbind(song_data,feature_data[1,])
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Wooly Bully"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}





# track name: Ballad of the Green Berets  
# artist: SSgt. Barry Sadler 
# year: 1966 

song_list <- get_track("Barry Sadler")
feature_list <- get_features("Barry Sadler") 
song_data <- as.tibble(song_list) %>% filter(track_name=="The Ballad Of The Green Berets")
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data)
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Ballad of the Green Berets"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}




# track name: Sugar, Sugar 
# artist: The Archies       
# year: 1969 

song_list <- get_track("The Archies")
feature_list <- get_features("The Archies") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Sugar, Sugar")
song_data
# the song does not exist in the Spotify API with the band name "The Archies"
# But Spotify has features from Ron Dante (the lead singer)
song_list <- get_track("Ron Dante")
feature_list <- get_features("Ron Dante") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Sugar Sugar") %>% slice(1)
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data)
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Sugar, Sugar"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}


# track name: Tie a Yellow Ribbon Round the Ole Oak Tree
# artist:Tony Orlando and Dawn
# year: 1973

song_list <- get_track("Tony Orlando and Dawn")
feature_list <- get_features("Tony Orlando and Dawn") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Tie a Yellow Ribbon Round the Ole Oak Tree")
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data)
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Tie a Yellow Ribbon Round the Ole Oak Tree"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}


# track name: My Sharona
# artist:The Knack
# year: 1979

song_list <- get_track("The Knack")
feature_list <- get_features("The Knack") 
song_data <- as.tibble(song_list) %>% filter(track_name=="My Sharona")
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data[1,]) %>% slice(1)
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "My Sharona"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}


# track name: Careless Whisper
# artist: Wham! featuring George Michael
# year: 1985

song_list <- get_track("Wham")
feature_list <- get_features("Wham") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Careless Whisper") %>% slice(1)
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data[1,])
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Careless Whisper"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}

# track name: That's What Friends Are For
# artist: Dionne and Friends (Dionne Warwick, Gladys Knight, Elton John)
# year: 1986

song_list <- get_track("Dionne Warwick")
feature_list <- get_features("Dionne Warwick") 
song_data <- as.tibble(song_list) %>% filter(track_name=="That's What Friends Are For") %>% slice(1)
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data)
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "That's What Friends Are For"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}



# track name: Gangsta's Paradise
# artist: Coolio featuring L.V.
# year:  1995 

song_list <- get_track("Coolio")
feature_list <- get_features("Coolio") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Gangsta's Paradise") %>% slice(1)
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data[1,])
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Gangsta's Paradise"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}






# track name: Macarena (Bayside Boys Mix) 
# artist:Los del Río  
# year: 1996

song_data <- as.tibble(spotify_track_data) %>% 
  filter(track_name=="Macarena - Bayside Boys Remix") %>% 
  slice(1)
get_feature_data <- song_data
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Macarena (Bayside Boys Mix)"){
    list$track_uri[i] <- str_sub(get_feature_data$track_id)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}






# track name: Yeah!
# artist:Usher featuring Lil Jon and Ludacris 
# year: 2004

song_list <- get_track("Usher")
feature_list <- get_features("Usher") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Yeah!") %>% slice(1)
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data[1,])
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Yeah!"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}



# track name: Low
# artist: Flo Rida featuring T-Pain   
# year: 2008

song_list <- get_track("Flo Rida")
feature_list <- get_features("Flo Rida") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Low (feat T-Pain)")
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri) %>% slice(1)
get_feature_data <- cbind(song_data,feature_data[1,])
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Low"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}




# track name: Somebody That I Used to Know   
# artist:Gotye featuring Kimbra 
# year: 2012 

song_list <- get_track("Gotye")
feature_list <- get_features("Gotye") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Somebody That I Used To Know") %>% slice(1)
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data[1,])
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Somebody That I Used to Know"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}



# track name: Thrift Shop 
# artist:Macklemore and Ryan Lewis featuring Wanz 
# year: 2015 

song_list <- get_track("Ryan Lewis")
feature_list <- get_features("Ryan Lewis") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Thrift Shop (feat. Wanz)") %>% slice(1)
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data[1,])
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Thrift Shop"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}


# track name: Uptown Funk
# artist: Mark Ronson featuring Bruno Mars  
# year: 2015 

song_list <- get_track("Mark Ronson")
feature_list <- get_features("Mark Ronson") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Uptown Funk") %>% slice(1)
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data[1,])
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Uptown Funk"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}



# track name: Love Yourself 
# artist:Justin Bieber
# year: 2016


song_list <- get_track("Justin Bieber")
feature_list <- get_features("Justin Bieber") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Love Yourself") %>% slice(1)
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data[1,])
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Love Yourself"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}




# track name:Shape of You 
# artist:Ed Sheeran   
# year: 2017

song_list <- get_track("Ed Sheeran")
feature_list <- get_features("Ed Sheeran") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Shape of You") 
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data[1,])
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Shape of You"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}


# track name: Perfect
# artist: Ed Sheeran
# year: 2018

song_list <- get_track("Ed Sheeran")
feature_list <- get_features("Ed Sheeran") 
song_data <- as.tibble(song_list) %>% filter(track_name=="Perfect")
feature_data <- as.tibble(feature_list) %>% filter(track_uri==song_data$track_uri)
get_feature_data <- cbind(song_data,feature_data)
for (i in 1:nrow(list)) {
  if (list$track_name[i] == "Perfect"){
    list$track_uri[i] <- str_sub(get_feature_data$track_uri)
    list$danceability[i] <- str_sub(get_feature_data$danceability)
    list$energy[i] <- str_sub(get_feature_data$energy)
    list$loudness[i] <- str_sub(get_feature_data$loudness)
    list$speechiness[i] <- str_sub(get_feature_data$speechiness)
    list$acousticness[i] <- str_sub(get_feature_data$acousticness)
    list$instrumentalness[i] <- str_sub(get_feature_data$instrumentalness)
    list$liveness[i] <- str_sub(get_feature_data$liveness)
    list$valence[i] <- str_sub(get_feature_data$valence)
    list$tempo[i] <- str_sub(get_feature_data$tempo)
    list$duration_ms[i] <- str_sub(get_feature_data$duration_ms)
  }
}
```


#Final complete dataset
- Final dataset Billboard_Top_1.csv including all songs and features from top 1 song of the Billboard Hot 100 song list from 1960 to 2018.  

```{r,eval=FALSE}
List <- write.csv(list,"Billboard_Top_1.csv")
```

#Data Analysis

- The final dataset include 10 specific audio features (danceability, energy, loudness, speechiness, acousticness, instrumentalness, liveness, valence, tempo, duration)  
```{r,echo=FALSE}
top <- read.csv("Billboard_Top_1.csv")
colnames(top)
```

# Barplot and basic statistics of different audio features 

```{r,echo=FALSE}
barplot(top$danceability,
        main = "Danceability vs. Year",
        xlab = "Year", 
        ylab = "Danceability")
summary(top$danceability)

barplot(top$energy,
        main = "Energy vs. Year",
        xlab = "Year", 
        ylab = "Energy")
summary(top$energy)

barplot(top$loudness,
        main = "Loudness vs. Year",
        xlab = "Year", 
        ylab = "Loudness")
summary(top$loudness)

barplot(top$speechiness,
        main = "Speechiness vs. Year",
        xlab = "Year", 
        ylab = "Speechiness")
summary(top$speechiness)

barplot(top$acousticness,
        main = "Acousticness vs. Year",
        xlab = "Year", 
        ylab = "Acousticness")
summary(top$acousticness)

barplot(top$instrumentalness,
        main = "Instrumentalness vs. Year",
        xlab = "Year", 
        ylab = "Instrumentalness")
summary(top$instrumentalness)

barplot(top$liveness,
        main = "Liveness vs. Year",
        xlab = "Year", 
        ylab = "Liveness")
summary(top$liveness)

barplot(top$valence,
        main = "Valence vs. Year",
        xlab = "Year", 
        ylab = "Valence")
summary(top$valence)

barplot(top$tempo,
        main = "Tempo vs. Year",
        xlab = "Year", 
        ylab = "Tempo")
summary(top$tempo)

barplot(top$duration_ms,
        main = "Duration vs. Year",
        xlab = "Year", 
        ylab = "Duration")
summary(top$duration_ms)
```

##Description: 
Initial data analysis include plotting individual histograms of individual features and looking at basic statistical analysis (min, median, mean, max). 


##Analysis: 

Danceability has average around 0.64, which is pretty high on the scale. This proves that on average, music that are rhythmic, upbeat- suitable for dancing - tends to be more popular among people. Similarly, energy has average around 0.59, which shows that music lovers prefer songs that consist faster tempo. The average value for tempo is 116, which in music is Allegretto - moderately fast. This also points to the same result as people tend to like more energetic/faster songs. Loudness has average -8.5, this is considerably low on the decibel level, proving that popular music tends to be not too loud. Speechiness and acousticness both has average value that are low on the scale, showing that people prefer songs that include less electronic sounds and more vocal lines. Instrumentalness and liveness both have pretty low values. This makes sense as most of the songs on the list are recorded in a studio, and are songs instead of symphonic works. The mean and median duration of songs are pretty close together- both have values around 230000 milliseconds (around 3.83 min). 

The most interesting analytics comes with the feature “valence”. This is the only feature of the entire dataset that involves user input data. The other features are calculated from the tempo, sound quality of the songs themselves, while valence is calculated through user ratings (labeling whether the song feels happy or sad), user ratings do depends on all the previous audio features ( i.e. danceability, loudness, energy etc). On the histogram for valence, the max value is 0.97, and the min value is 0.1, with average value 0.60. Valence values varies a lot through the 59 songs, and the max and min each stands at the opposite ends of the spectrum. 

The rest of the project focus on deeper analysis of valence values. 

###Changes to dataset values (grouping variable year)
- Change year index so that year would be displayed as by decade instead of just one single year. (this is specifically created for the ggplot analysis later, since I don't want continuous scale for the legend.)

```{r,echo=FALSE}
for (i in 1:nrow(top)) { 
  if (top$year[i] < 1970) {
    top$year[i] <- str_sub("1960-1969") 
  }else if (top$year[i] < 1980){
    top$year[i] <- str_sub("1970-1979") 
    
  }else if (top$year[i] < 1990){
    top$year[i] <- str_sub("1980-1989") 
  
  }else if (top$year[i] < 2000){
    top$year[i] <- str_sub("1990-1999")
 
  }else if (top$year[i] < 2010){
    top$year[i] <- str_sub("2000-2009")
  
  }else if (top$year[i] < 2019){
    top$year[i] <- str_sub("2009-2018")
  }
}
```

#GGplot analysis for each audio feature vs. valence

```{r,echo=FALSE}
ggplot(top, aes(danceability,valence)) +
  geom_point(aes(color = as.factor(year))) +
  geom_smooth(se = FALSE) +
  labs(title = "Danceability vs. Valence")


ggplot(top, aes(top$energy,top$valence)) +
  geom_point(aes(color = as.factor(year))) +
  geom_smooth(se = FALSE) +
  labs(title = "Energy vs. Valence")

ggplot(top, aes(top$loudness,top$valence)) +
  geom_point(aes(color = as.factor(year))) +
  geom_smooth(se = FALSE) +
  labs(title = "Loudness vs. Valence")

ggplot(top, aes(top$speechiness,top$valence)) +
  geom_point(aes(color = as.factor(year))) +
  geom_smooth(se = FALSE) +
  labs(title = "Speechiness vs. Valence")

ggplot(top, aes(top$acousticness,top$valence)) +
  geom_point(aes(color = as.factor(year))) +
  geom_smooth(se = FALSE) +
  labs(title = "Acousticness vs. Valence")


ggplot(top, aes(top$tempo,top$valence)) +
  geom_point(aes(color = as.factor(year))) +
  geom_smooth(se = FALSE) +
  labs(title = "Tempo vs. Valence")

ggplot(top, aes(top$liveness,top$valence)) +
  geom_point(aes(color = as.factor(year))) +
  geom_smooth(se = FALSE) +
  labs(title = "Liveness vs. Valence")

ggplot(top, aes(top$duration_ms,top$valence)) +
  geom_point(aes(color = as.factor(year))) +
  geom_smooth(se = FALSE) +
  labs(title = "Duration vs. Valence")
```


##Description: 
Focusing on analysis of valence level. Plots consist of each individual audio feature graphed on the x-axis, and valence graphed on the y-axis. 

##Analysis: 
When danceability, energy, tempo increases, valence increases. While for loudness, speechiness, acousticness, and duration of songs, the higher they are on the scale, the lower the valence score. 
According to the graphs above, songs that are determined as “happy” are generally songs that are fast, upbeat, and not so loud. While songs that fits the “sad” or more negative mood are songs that are slower, longer, and includes more spoken words. 

# Scatterplot of different audio features
- The scatter plots are used to visualize dataset after filtering out certain valence values (see shiny app for more analysis)

```{r,echo=FALSE}

par(mfrow=c(2,5))
plot(top$valence,top$danceability)
plot(top$valence,top$energy)
plot(top$valence,top$loudness)
plot(top$valence,top$speechiness)
plot(top$valence,top$liveness)
plot(top$valence,top$tempo)
plot(top$valence,top$duration_ms)
plot(top$valence,top$acousticness)
plot(top$valence,top$instrumentalness)
```


# Individual audio feature time series plot 
```{r,echo=FALSE}
top_orig <- read.csv("Billboard_Top_1.csv")
ggplot(top, aes(fill=top$year, y=top_orig$danceability, x=top_orig$year)) + 
  geom_line(color="purple") +
  geom_point()


ggplot(top, aes(fill=top$year, y=top_orig$energy, x=top_orig$year)) + 
  geom_line(color="purple") +
  geom_point()

ggplot(top, aes(fill=top$year, y=top_orig$loudness, x=top_orig$year)) + 
  geom_line(color="purple") +
  geom_point()

ggplot(top, aes(fill=top$year, y=top_orig$speechiness, x=top_orig$year)) + 
  geom_line(color="purple") +
  geom_point()


ggplot(top, aes(fill=top$year, y=top_orig$acousticness, x=top_orig$year)) + 
  geom_line(color="purple") +
  geom_point()


ggplot(top, aes(fill=top$year, y=top_orig$instrumentalness, x=top_orig$year)) + 
  geom_line(color="purple") +
  geom_point()


ggplot(top, aes(fill=top$year, y=top_orig$acousticness, x=top_orig$year)) + 
  geom_line(color="purple") +
  geom_point()


ggplot(top, aes(fill=top$year, y=top_orig$tempo, x=top_orig$year)) + 
  geom_line(color="purple") +
  geom_point()

ggplot(top, aes(fill=top$year, y=top_orig$duration_ms, x=top_orig$year)) + 
  geom_line(color="purple") +
  geom_point()
```

## Description: 
Focusing on analysis of individual audio features. Time series plot shows the change of values across time. 

## Analysis: 
The features that seems to follow an increasing trend is loudness. This shows that as time goes on, popular music tends to be quieter (as the y-axis goes from -60 to 0, the closer loudness to 0, the quieter the songs are).

Similarly with speechiness. The songs between 2000 and 2018 seems to have many spikes in  speechiness graphs. One reason could be the increasing popularity in rap music.

Duration is another feature that slightly increased with time. As time went on, music got a little longer (although not by a lot). 

Instrumentalness seems to decrease over time. Between 1960 to 1965, music tends to have more instrumental preludes and interludes, while after 1965, popular music tends to have more lyrics and vocal lines.

All the other features seems to follow a pretty constant trend over time.


# Detailed analysis of the valence time series plot
```{r,echo=FALSE}
ggplot(top, aes(fill=top$year, y=top_orig$valence, x=top_orig$year)) + 
  geom_line(color="purple") +
  geom_point()
```


## Description: 
Focusing on analysis of valence level. Time series plot shows the change of valence values across time. 

## Analysis: 
While all the songs are listed as number one sound tracks from each year’s hot billboard song ratings. Valence scores varies a lot between each year for each song track.  

The happiest song (highest valence score) belongs to the song track “Joy to the World” by Three Dog Night in 1971. The score has a valence score of 0.97, with danceability 0.591, energy 0.726, tempo around 130. So overall, the song is fast, energetic, and suitable for dancing. 

The saddest song (lowest valence score) belongs to the song track “I Will Always Love You” by Whitney Houston in 1993. The score has a valence score of 0.108, with danceability 0.306, energy 0.214, tempo 67. A significant drop in feature values from “Joy to the World”. It is a slower song, with less energy ratings. Since it’s valence rating is almost 0, this song expresses a sad emotion. 



\newpage

#Conclusion: 
```{r}

happy <- subset(top_orig,top_orig$valence > 0.95) %>% select("track_name","artist","year","danceability",                                          "energy","loudness","speechiness","acousticness",
       "instrumentalness","liveness","valence","tempo","duration_ms")


sad <- subset(top_orig,top_orig$valence<0.15)%>% select("track_name","artist","year","danceability",                                                       "energy","loudness","speechiness","acousticness",
                    "instrumentalness","liveness","valence","tempo","duration_ms")

happy
sad
```


After filtering out and analyzing songs with valence score above 0.95 (happy) and valence score below 0.15 (sad), the project concludes with the following analysis: 

A happy song generally has danceability rating above 0.6, energy above 0.7, loudness around -10, speechiness around 0.06, acousticness around 0.23, and tempo around 130. So if a song is fast, not too loud, have a good balance between instrumentation and vocal lyrics, it can usually stimulate happiness within human emotions, and would make people want to dance to the tune.

A sad song generally has danceability rating below 0.3, energy below 0.2, loudness around -15, speechiness around 0.03, acousticness around 0.80, and tempo around 60. So when we hear a song that generally involves slower tempo, filled with sounds that are more acoustic (as oppose to electronic), and with less spoken words/lyrics, it usually feels like a sad song. 

Interestingly, sad songs seems to score higher in loudness than happy songs. Intuitively, sad songs should be quieter as they generally expresses melancholy and sorrow. Yet, some songs (like “I will always love you” Whitney Houston, which scored the lowest on valence level) gets loud during its climatic section, expressing heartfelt passions and emotions. 

So next time, when you need to select a song that fits a certain mood, be sure to check out its audio features and valence scores! 


